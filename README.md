# Adversarial Robustness of NLP Models: Evaluating Prompt Injection and Backdoor Attacks

This project evaluates the adversarial robustness of NLP models, focusing on prompt injection attacks and backdoor vulnerabilities.

- **Paper Outline v1**: [Download PDF](https://github.com/52147/Adversarial-Robustness-of-NLP-Models-Evaluating-Prompt-Injection-and-Backdoor-Attacks/blob/main/Adversarial_NLP_Security_Report_Outline_v1.pdf.pdf)  
- **Experiments on GPT-4, LLaMA, and Alpaca**  
- **70% Success Rate in Bypassing Safety Filters**  
- **Defensive Fine-tuning with Adversarial Data**

## Overview
This repository contains:
- **Attack Implementations**: Code for generating adversarial attacks (prompt injection, backdoors)
- **Defense Strategies**: Methods for improving robustness
- **Experimental Results**: Evaluation of popular LLMs

## ðŸ“‚ Files & Directories
- `notebooks/` - Jupyter notebooks with attack/defense experiments
- `data/` - Adversarial datasets used for fine-tuning
- `models/` - Pre-trained adversarially fine-tuned models

---

- **Read more details in the [Paper Outline v1](https://github.com/52147/Adversarial-Robustness-of-NLP-Models-Evaluating-Prompt-Injection-and-Backdoor-Attacks/blob/main/Adversarial_NLP_Security_Report_Outline_v1.pdf.pdf).**  
- Feel free to reach out if you're interested in collaborating!
